---
controls:
version: rh-1.8
id: 4
text: "Worker Nodes"
type: "node"
groups:
  - id: 4.1
    text: "Worker Node Configuration Files"
    checks:
      - id: 4.1.1
        text: "Ensure that the kubelet service file permissions are set to 644 or more restrictive (Automated)"
        audit: |
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          oc debug node/$NODE_NAME -- chroot /host stat -c "$NODE_NAME %n permissions=%a" /etc/systemd/system/kubelet.service 2> /dev/null
        tests:
          test_items:
            - flag: "permissions"
              compare:
                op: bitmask
                value: "644"
        remediation: |
          By default, the kubelet service file has permissions of 644.
        scored: true

      - id: 4.1.2
        text: "Ensure that the kubelet service file ownership is set to root:root (Automated)"
        audit: |
          # Should return root:root for each node
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          oc debug node/$NODE_NAME -- chroot /host stat -c "$NODE_NAME %n %U:%G" /etc/systemd/system/kubelet.service 2> /dev/null
        tests:
          test_items:
            - flag: root:root
        remediation: |
          By default, the kubelet service file has ownership of root:root.
        scored: true

      - id: 4.1.3
        text: "If proxy kube proxy configuration file exists ensure permissions are set to 644 or more restrictive (Manual)"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')
          # Get the pod name in the openshift-sdn namespace
          POD_NAME=$(oc get pods -n openshift-sdn -l app=sdn --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
          echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-sdn "$POD_NAME"  -- stat -Lc "$i %n permissions=%a" /config/kube-proxy-config.yaml  2>/dev/null
          fi
        tests:
          bin_op: or
          test_items:
            - flag: "permissions"
              set: true
              compare:
                op: bitmask
                value: "644"
        remediation: |
          None needed.
        scored: false

      - id: 4.1.4
        text: "If proxy kubeconfig file exists ensure ownership is set to root:root (Manual)"
        audit: |
          # Get the node name where the pod is running
          NODE_NAME=$(oc get pod "$HOSTNAME" -o=jsonpath='{.spec.nodeName}')
          # Get the pod name in the openshift-sdn namespace
          POD_NAME=$(oc get pods -n openshift-sdn -l app=sdn --field-selector spec.nodeName="$NODE_NAME" -o jsonpath='{.items[0].metadata.name}' 2>/dev/null)

          if [ -z "$POD_NAME" ]; then
          echo "No matching pods found on the current node."
          else
          # Execute the stat command
          oc exec -n openshift-sdn "$POD_NAME"  -- stat -Lc "$i %n %U:%G" /config/kube-proxy-config.yaml  2>/dev/null
          fi
        use_multiple_values: true
        tests:
          bin_op: or
          test_items:
            - flag: root:root
        remediation: |
          None required. The configuration is managed by OpenShift operators.
        scored: false

      - id: 4.1.5
        text: "Ensure that the --kubeconfig kubelet.conf file permissions are set to 644 or more restrictive (Automated)"
        audit: |
          # Check permissions
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          oc debug node/$NODE_NAME -- chroot /host stat -c "$NODE_NAME %n permissions=%a" /etc/kubernetes/kubelet.conf 2> /dev/null
        use_multiple_values: true
        tests:
          test_items:
            - flag: "permissions"
              compare:
                op: bitmask
                value: "644"
        remediation: |
          None required.
        scored: true

      - id: 4.1.6
        text: "Ensure that the --kubeconfig kubelet.conf file ownership is set to root:root (Automated)"
        audit: |
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          oc debug node/$NODE_NAME -- chroot /host stat -c "$NODE_NAME %n %U:%G" /etc/kubernetes/kubelet.conf 2> /dev/null
        use_multiple_values: true
        tests:
          test_items:
            - flag: root:root
        remediation: |
          None required.
        scored: true

      - id: 4.1.7
        text: "Ensure that the certificate authorities file permissions are set to 644 or more restrictive (Automated)"
        audit: |
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          oc debug node/$NODE_NAME -- chroot /host stat -c "$NODE_NAME %n permissions=%a" /etc/kubernetes/kubelet-ca.crt 2> /dev/null
        use_multiple_values: true
        tests:
          test_items:
            - flag: "permissions"
              compare:
                op: bitmask
                value: "644"
        remediation: |
          None required.
        scored: true

      - id: 4.1.8
        text: "Ensure that the client certificate authorities file ownership is set to root:root (Automated)"
        audit: |
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          oc debug node/$NODE_NAME -- chroot /host stat -c "$NODE_NAME %n %U:%G" /etc/kubernetes/kubelet-ca.crt 2> /dev/null
        use_multiple_values: true
        tests:
          test_items:
            - flag: root:root
        remediation: |
          None required.
        scored: true

      - id: 4.1.9
        text: "Ensure that the kubelet --config configuration file has permissions set to 600 or more restrictive (Automated)"
        audit: |
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          # default setups have the file present at /var/lib/kubelet only. Custom setup is present at /var/data/kubelet/config.json.
          oc debug node/$NODE_NAME -- /bin/sh -c '
          if [ -f /var/data/kubelet/config.json ]; then
            chroot /host stat -c "$NODE_NAME %n permissions=%a" /var/data/kubelet/config.json;
          else
            chroot /host stat -c "$NODE_NAME %n permissions=%a" /var/lib/kubelet/config.json;
          fi' 2> /dev/null
        use_multiple_values: true
        tests:
          test_items:
            - flag: "permissions"
              compare:
                op: bitmask
                value: "600"
        remediation: |
          None required.
        scored: true

      - id: 4.1.10
        text: "Ensure that the kubelet configuration file ownership is set to root:root (Automated)"
        audit: |
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          # default setups have the file present at /var/lib/kubelet only. Custom setup is present at /var/data/kubelet/config.json.
          oc debug node/$NODE_NAME -- /bin/sh -c '
          if [ -f /var/data/kubelet/config.json ]; then
            chroot /host stat -c "$NODE_NAME %n %U:%G" /var/data/kubelet/config.json;
          else
            chroot /host stat -c "$NODE_NAME %n %U:%G" /var/lib/kubelet/config.json;
          fi' 2> /dev/null
        use_multiple_values: true
        tests:
          test_items:
            - flag: root:root
        remediation: |
          None required.
        scored: true

  - id: 4.2
    text: "Kubelet"
    checks:
      - id: 4.2.1
        text: "Activate Garbage collection in OpenShift Container Platform 4, as appropriate (Manual)"
        audit: |
          echo "Retrieving and inspecting garbage collection configuration from node-local kubelet configz..."
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          oc get --raw /api/v1/nodes/$NODE_NAME/proxy/configz | jq '.kubeletconfig'
        tests:
          test_items:
            - flag: "evictionHard"
            - flag: "imageGCHighThresholdPercent"
            - flag: "imageGCLowThresholdPercent"
            - flag: "imageMinimumGCAge"
        remediation: |
          OpenShift manages node garbage collection through KubeletConfig custom resources per MachineConfigPool.
          To configure or adjust garbage collection thresholds, follow the documentation:
          https://docs.openshift.com/container-platform/latest/nodes/nodes/nodes-nodes-garbage-collection.html
          Example: Create or modify a KubeletConfig object to include:
          ---
          evictionHard:
            "memory.available": "200Mi"
            "nodefs.available": "10%"
            "imagefs.available": "15%"
          imageGCHighThresholdPercent: 85
          imageGCLowThresholdPercent: 80
          imageMinimumGCAge: "2m0s"
          Then apply the `KubeletConfig` to the appropriate `MachineConfigPool`.
        scored: true

      - id: 4.2.2
        text: "Ensure that the --anonymous-auth argument is set to false (Automated)"
        audit: |
          echo "Checking if anonymous-auth is disabled in kubelet configuration on the current node..."
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          oc get --raw /api/v1/nodes/$NODE_NAME/proxy/configz | jq '.kubeletconfig.authentication.anonymous.enabled'
        tests:
          test_items:
            - flag: "false"
        remediation: |
          By default, OpenShift sets anonymous-auth to false in Kubelet configuration.
          If this value is found to be true, create or patch a KubeletConfig object with:
          ---
          kind: KubeletConfig
          apiVersion: machineconfiguration.openshift.io/v1
          metadata:
            name: disable-anonymous-auth
          spec:
            kubeletConfig:
              authentication:
                anonymous:
                  enabled: false
          Then apply this KubeletConfig to the appropriate MachineConfigPool.
          See OpenShift documentation on configuring node-level security settings.
        scored: true

      - id: 4.2.3
        text: "Ensure that the --authorization-mode argument is not set to AlwaysAllow (Automated)"
        type: manual
        # Takes a lot of time for connection to fail and
        audit: |
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          oc get --raw /api/v1/nodes/$NODE_NAME/proxy/configz | jq '.kubeletconfig.authorization' 2> /dev/null
        use_multiple_values: true
        tests:
          test_items:
            - flag: mode
              compare:
                op: noteq
                value: AlwaysAllow
        remediation: |
          None required. Unauthenticated/Unauthorized users have no access to OpenShift nodes.
        scored: true

      - id: 4.2.4
        text: "Ensure that the --client-ca-file argument is set as appropriate (Automated)"
        audit: |
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          oc get --raw /api/v1/nodes/$NODE_NAME/proxy/configz | jq -r '.kubeletconfig.authentication.x509.clientCAFile' 2> /dev/null
        use_multiple_values: true
        tests:
          test_items:
            - flag: '/etc/kubernetes/kubelet-ca.crt'
        remediation: |
          None required. Changing the clientCAFile value is unsupported.
        scored: true

      - id: 4.2.5
        text: "Verify that the read only port is not used or is set to 0 (Automated)"
        audit: |
          oc -n openshift-kube-apiserver get cm config -o json | jq -r '.data."config.yaml"' | jq -r '.apiServerArguments."kubelet-read-only-port"[]'  2> /dev/null
        tests:
          test_items:
            - flag: '0'
        remediation: |
          In earlier versions of OpenShift 4, the read-only-port argument is not used.
          Follow the instructions in the documentation https://docs.openshift.com/container-platform/latest/post_installation_configuration/machine-configuration-tasks.html#create-a-kubeletconfig-crd-to-edit-kubelet-parameters_post-install-machine-configuration-tasks
          to create a kubeletconfig CRD and set the kubelet-read-only-port is set to 0.
        scored: true

      - id: 4.2.6
        text: "Ensure that the --streaming-connection-idle-timeout argument is not set to 0 (Automated)"
        audit: |
          # Should return 1 for node
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          echo streamingConnectionIdleTimeout=$(oc get --raw /api/v1/nodes/$NODE_NAME/proxy/configz | jq '.kubeletconfig.streamingConnectionIdleTimeout' 2> /dev/null)
        use_multiple_values: true
        tests:
          test_items:
            - flag: streamingConnectionIdleTimeout
              compare:
                op: noteq
                value: 0s
        remediation: |
          Follow the instructions https://docs.openshift.com/container-platform/latest/post_installation_configuration/machine-configuration-tasks.html#create-a-kubeletconfig-crd-to-edit-kubelet-parameters_post-install-machine-configuration-tasks in the documentation to create a kubeletconfig CRD and set
          the streamingConnectionIdleTimeout to the desired value. Do not set the value to 0.
        scored: true

      - id: 4.2.7
        text: "Ensure that the --make-iptables-util-chains argument is set to true (Manual)"
        audit: |
          # Should return 1 for node
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          oc get --raw /api/v1/nodes/$NODE_NAME/proxy/configz | jq '.kubeletconfig.makeIPTablesUtilChains' 2> /dev/null
        use_multiple_values: true
        tests:
          test_items:
            - flag: "true"
        remediation: |
          None required. The makeIPTablesUtilChains argument is set to true by default.
        scored: false

      - id: 4.2.8
        text: "Ensure that the kubeAPIQPS [--event-qps] argument is set to 0 or a level which ensures appropriate event capture (manual)"
        audit: |
          echo "Checking 'kubeAPIQPS' setting in Kubelet config on current node..."
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          oc get --raw /api/v1/nodes/$NODE_NAME/proxy/configz \
            | jq '.kubeletconfig'
        tests:
          test_items:
            - path: ".kubeAPIQPS"
              compare:
                op: gte
                value: 1
        remediation: |
          OpenShift sets kubeAPIQPS to a default of 50, which is appropriate in most environments.
          If kubeAPIQPS is set to 0, event rate limiting is disabled, which can overwhelm the kubelet with excessive events.
          To configure a proper limit, create or modify a `KubeletConfig` resource with an appropriate value:
          ---
          apiVersion: machineconfiguration.openshift.io/v1
          kind: KubeletConfig
          metadata:
            name: set-kubeapiqps
          spec:
            kubeletConfig:
              kubeAPIQPS: 50
        scored: true

      - id: 4.2.9
        text: "Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate (Manual)"
        audit: |
          oc get configmap config -n openshift-kube-apiserver -ojson | \
            jq -r '.data["config.yaml"]' | \
            jq -r '.apiServerArguments | ."kubelet-client-certificate"[0], ."kubelet-client-key"[0]' 2> /dev/null
        tests:
          bin_op: and
          test_items:
            - flag: "/etc/kubernetes/static-pod-certs/secrets/kubelet-client/tls.crt"
            - flag: "/etc/kubernetes/static-pod-certs/secrets/kubelet-client/tls.key"
        remediation: |
          No remediation is required. OpenShift manages secure TLS connections to kubelets by default using its internal certificate authority.
          These X.509 certificates are rotated and validated automatically by the platform.
          Manual modifications to the TLS paths or keys are not supported and can lead to cluster issues.
        scored: true

      - id: 4.2.10
        text: "Ensure that the --rotate-certificates argument is not set to false (Manual)"
        audit: |
          echo "Checking 'rotateCertificates' setting in Kubelet config on current node..."
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          oc get --raw /api/v1/nodes/$NODE_NAME/proxy/configz | jq '.kubeletconfig.rotateCertificates'
        use_multiple_values: true
        tests:
          test_items:
            - flag: ".rotateCertificates"
              compare:
                op: eq
                value: true
        remediation: |
          No remediation required. By default, OpenShift enables certificate rotation via rotateCertificates=true.
          If disabled, you must either enable rotation via KubeletConfig or implement external certificate renewal.
          Example remediation using KubeletConfig:
          ---
          apiVersion: machineconfiguration.openshift.io/v1
          kind: KubeletConfig
          metadata:
            name: enable-cert-rotation
          spec:
            kubeletConfig:
              rotateCertificates: true
        scored: true

      - id: 4.2.11
        text: "Verify that the RotateKubeletServerCertificate argument is set to true (manual)"
        audit: |
          echo "Checking that RotateKubeletServerCertificate is enabled in kubelet config on current node..."
          NODE_NAME=$(oc get pod $HOSTNAME -o=jsonpath='{.spec.nodeName}')
          echo "Verifying feature gate: RotateKubeletServerCertificate"
          oc get --raw /api/v1/nodes/$NODE_NAME/proxy/configz \
            | jq '.kubeletconfig.featureGates.RotateKubeletServerCertificate'
          echo "Verifying that certificate rotation is enabled"
          oc get --raw /api/v1/nodes/$NODE_NAME/proxy/configz \
            | jq '.kubeletconfig.rotateCertificates'
        tests:
          bin_op: and
          test_items:
            - flag: "RotateKubeletServerCertificate"
              compare:
                op: eq
                value: true
            - flag: "rotateCertificates"
              compare:
                op: eq
                value: true
        remediation: |
          No remediation is required. OpenShift enables RotateKubeletServerCertificate by default and manages certificate rotation automatically.
          If the feature gate or rotation setting is disabled, configure a `KubeletConfig` CR and apply it to the MachineConfigPool:
          ---
          apiVersion: machineconfiguration.openshift.io/v1
          kind: KubeletConfig
          metadata:
            name: enable-server-cert-rotation
          spec:
            kubeletConfig:
              rotateCertificates: true
              featureGates:
                RotateKubeletServerCertificate: true
        scored: true

      - id: 4.2.12
        text: "Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers (Manual)"
        audit: |
          # needs verification
          # verify cipher suites
          oc get --namespace=openshift-ingress-operator ingresscontroller/default -o json | jq '.status.tlsProfile.ciphers' 2> /dev/null
          oc get kubeapiservers.operator.openshift.io cluster -o json | jq '.spec.observedConfig.servingInfo.cipherSuites' 2> /dev/null
          oc get openshiftapiservers.operator.openshift.io cluster -o json | jq '.spec.observedConfig.servingInfo.cipherSuites' 2> /dev/null
          oc get cm -n openshift-authentication v4-0-config-system-cliconfig -o jsonpath='{.data.v4\-0\-config\-system\-cliconfig}' | jq '.servingInfo.cipherSuites' 2> /dev/null
          #check value for tlsSecurityProfile; null is returned if default is used
          oc get kubeapiservers.operator.openshift.io cluster -o json |jq .spec.tlsSecurityProfile 2> /dev/null
        type: manual
        remediation: |
          Follow the directions above and in the OpenShift documentation to configure the tlsSecurityProfile.
          Configuring Ingress. https://docs.openshift.com/container-platform/4.15/networking/ingress-operator.html#nw-ingress-controller-configuration-parameters_configuring-ingress
          Please reference the OpenShift TLS security profile documentation for more detail on each profile.
          https://docs.openshift.com/container-platform/4.15/security/tls-security-profiles.html
        scored: false
