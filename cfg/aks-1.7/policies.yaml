---
controls:
version: "aks-1.7"
id: 4
text: "Policies"
type: "policies"
groups:
  - id: 4.1
    text: "RBAC and Service Accounts"
    checks:
      - id: 4.1.1
        text: "Ensure that the cluster-admin role is only used where required (Automated)"
        audit: "kubectl get clusterrolebindings -o=custom-columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name"
        audit_config: "kubectl get clusterrolebindings"
        tests:
          test_items:
            - flag: cluster-admin
              path: '{.roleRef.name}'
              set: true
              compare:
                op: eq
                value: "cluster-admin"
        remediation: |
          Identify all clusterrolebindings to the cluster-admin role. Check if they are used and
          if they need this role or if they could use a role with fewer privileges.
          Where possible, first bind users to a lower privileged role and then remove the
          clusterrolebinding to the cluster-admin role :
          kubectl delete clusterrolebinding [name]
        scored: false

      - id: 4.1.2
        text: "Minimize access to secrets (Automated)"
        audit: "kubectl get roles,rolebindings --all-namespaces -o=custom-columns=NAME:.metadata.name,ROLE:.rules[*].resources,SUBJECT:.subjects[*].name"
        audit_config: "kubectl get roles --all-namespaces -o json"
        tests:
          test_items:
            - flag: secrets
              path: '{.rules[*].resources}'
              set: true
              compare:
                op: eq
                value: "secrets"
            - flag: get
              path: '{.rules[*].verbs}'
              set: true
              compare:
                op: contains
                value: "get"
            - flag: list
              path: '{.rules[*].verbs}'
              set: true
              compare:
                op: contains
                value: "list"
            - flag: watch
              path: '{.rules[*].verbs}'
              set: true
              compare:
                op: contains
                value: "watch"
        remediation: |
          Where possible, remove get, list and watch access to secret objects in the cluster.
        scored: false

      - id: 4.1.3
        text: "Minimize wildcard use in Roles and ClusterRoles (Automated)"
        audit: "kubectl get roles --all-namespaces -o yaml | grep '*'"
        audit_config: "kubectl get clusterroles -o yaml | grep '*'"
        tests:
          test_items:
            - flag: wildcard
              path: '{.rules[*].verbs}'
              compare:
                op: notcontains
                value: "*"
        remediation: |
          Where possible, replace any use of wildcards in clusterroles and roles with specific objects or actions.
          Review the roles and clusterroles across namespaces and ensure that wildcards are not used for sensitive actions.
          Update roles by specifying individual actions or resources instead of using "*".
        scored: false

      - id: 4.1.4
        text: "Minimize access to create pods (Automated)"
        audit: "kubectl get roles,rolebindings --all-namespaces -o=custom-columns=NAME:.metadata.name,ROLE:.rules[*].resources,SUBJECT:.subjects[*].name"
        audit_config: "kubectl get roles --all-namespaces"
        tests:
          test_items:
            - flag: pods
              path: '{.rules[*].resources}'
              set: true
              compare:
                op: eq
                value: "pods"
            - flag: create
              path: '{.rules[*].verbs}'
              set: true
              compare:
                op: contains
                value: "create"
        remediation: |
          Where possible, remove create access to pod objects in the cluster.
        scored: false

      - id: 4.1.5
        text: "Ensure that default service accounts are not actively used (Automated)"
        audit: "kubectl get serviceaccounts --all-namespaces -o custom-columns=NAME:.metadata.name,NAMESPACE:.metadata.namespace,TOKEN:.automountServiceAccountToken"
        audit_config: "kubectl get serviceaccounts --all-namespaces"
        tests:
          test_items:
            - flag: default
              path: '{.metadata.name}'
              set: true
              compare:
                op: eq
                value: "default"
            - flag: automountServiceAccountToken
              path: '{.automountServiceAccountToken}'
              set: true
              compare:
                op: eq
                value: "false"
        remediation: |
          Create explicit service accounts wherever a Kubernetes workload requires specific access
          to the Kubernetes API server.
          Modify the configuration of each default service account to include this value
          automountServiceAccountToken: false
        scored: false

      - id: 4.1.6
        text: "Ensure that Service Account Tokens are only mounted where necessary (Automated)"
        audit: "kubectl get pods --all-namespaces -o custom-columns=NAME:.metadata.name,NAMESPACE:.metadata.namespace,SERVICE_ACCOUNT:.spec.serviceAccountName,MOUNT_TOKEN:.spec.automountServiceAccountToken"
        audit_config: "kubectl get pods --all-namespaces"
        tests:
          test_items:
            - flag: automountServiceAccountToken
              path: '{.spec.automountServiceAccountToken}'
              set: true
              compare:
                op: eq
                value: "false"
        remediation: |
          Modify the definition of pods and service accounts which do not need to mount service
          account tokens to disable it.
        scored: false


  - id: 4.2
    text: "Pod Security Policies"
    checks:
      - id: 4.2.1
        text: "Minimize the admission of privileged containers (Automated)"
        audit: |
          kubectl get pods --all-namespaces -o json | \
            jq -r 'if any(.items[]?.spec.containers[]?; .securityContext?.privileged == true) then "PRIVILEGED_FOUND" else "NO_PRIVILEGED" end'
        tests:
          test_items:
            - flag: "NO_PRIVILEGED"
              set: true
              compare:
                op: eq
                value: "NO_PRIVILEGED"
        remediation: |
          Add a Pod Security Admission (PSA) policy to each namespace in the cluster to restrict the admission of privileged containers.
          To enforce a restricted policy for a specific namespace, use the following command:
          kubectl label --overwrite ns NAMESPACE pod-security.kubernetes.io/enforce=restricted
          You can also enforce PSA for all namespaces:
          kubectl label --overwrite ns --all pod-security.kubernetes.io/warn=baseline
          Additionally, review the namespaces that should be excluded (e.g., `kube-system`, `gatekeeper-system`, `azure-arc`, `azure-extensions-usage-system`) and adjust your filtering if necessary.
          To enable Pod Security Policies, refer to the detailed documentation for Kubernetes and Azure integration at:
          https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for-kubernetes
        scored: true

      - id: 4.2.2
        text: "Minimize the admission of containers wishing to share the host process ID namespace (Automated)"
        audit: |
          kubectl get pods --all-namespaces -o json | \
            jq -r 'if any(.items[]?; .spec.hostPID == true) then "HOSTPID_FOUND" else "NO_HOSTPID" end'
        tests:
          test_items:
            - flag: "NO_HOSTPID"
              set: true
              compare:
                op: eq
                value: "NO_HOSTPID"

        remediation: |
          Add a policy to each namespace in the cluster that restricts the admission of containers with hostPID. For namespaces that need it, ensure RBAC controls limit access to a specific service account.
          You can label your namespaces as follows to restrict or enforce the policy:
          kubectl label --overwrite ns NAMESPACE pod-security.kubernetes.io/enforce=restricted
          You can also use the following to warn about policies:
          kubectl label --overwrite ns --all pod-security.kubernetes.io/warn=baseline
          For more information, refer to the official Kubernetes and Azure documentation on policies:
          https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for-kubernetes
        scored: true

      - id: 4.2.3
        text: "Minimize the admission of containers wishing to share the host IPC namespace (Automated)"
        audit: |
          kubectl get pods --all-namespaces -o json | jq -r 'if any(.items[]?; .spec.hostIPC == true) then "HOSTIPC_FOUND" else "NO_HOSTIPC" end'
        tests:
          test_items:
            - flag: "NO_HOSTIPC"
              set: true
              compare:
                op: eq
                value: "NO_HOSTIPC"
        remediation: |
          Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostIPC containers.
          You can label your namespaces as follows to restrict or enforce the policy:
          kubectl label --overwrite ns NAMESPACE pod-security.kubernetes.io/enforce=restricted
          You can also use the following to warn about policies:
          kubectl label --overwrite ns --all pod-security.kubernetes.io/warn=baseline
          For more information, refer to the official Kubernetes and Azure documentation on policies:
          https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for-kubernetes
        scored: true

      - id: 4.2.4
        text: "Minimize the admission of containers wishing to share the host network namespace (Automated)"
        audit: |
          kubectl get pods --all-namespaces -o json | jq -r 'if any(.items[]?; .spec.hostNetwork == true) then "HOSTNETWORK_FOUND" else "NO_HOSTNETWORK" end'
        tests:
          test_items:
            - flag: "NO_HOSTNETWORK"
              set: true
              compare:
                op: eq
                value: "NO_HOSTNETWORK"
        remediation: |
          Add policies to each namespace in the cluster which has user workloads to restrict the admission of hostNetwork containers.
          You can label your namespaces as follows to restrict or enforce the policy:
          kubectl label --overwrite ns NAMESPACE pod-security.kubernetes.io/enforce=restricted
          You can also use the following to warn about policies:
          kubectl label --overwrite ns --all pod-security.kubernetes.io/warn=baseline
          For more information, refer to the official Kubernetes and Azure documentation on policies:
          https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for-kubernetes
        scored: true

      - id: 4.2.5
        text: "Minimize the admission of containers with allowPrivilegeEscalation (Automated)"
        audit: |
          kubectl get pods --all-namespaces -o json | \
            jq -r 'if any(.items[]?.spec.containers[]?; .securityContext?.allowPrivilegeEscalation == true) then "ALLOWPRIVILEGEESCALTION_FOUND" else "NO_ALLOWPRIVILEGEESCALTION" end'
        tests:
          test_items:
            - flag: "NO_ALLOWPRIVILEGEESCALTION"
              set: true
              compare:
                op: eq
                value: "NO_ALLOWPRIVILEGEESCALTION"
        remediation: |
          Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with .spec.allowPrivilegeEscalation set to true.
          You can label your namespaces as follows to restrict or enforce the policy:
          kubectl label --overwrite ns NAMESPACE pod-security.kubernetes.io/enforce=restricted
          You can also use the following to warn about policies:
          kubectl label --overwrite ns --all pod-security.kubernetes.io/warn=baseline
          For more information, refer to the official Kubernetes and Azure documentation on policies:
          https://learn.microsoft.com/en-us/azure/governance/policy/concepts/policy-for-kubernetes
        scored: true


  - id: 4.3
    text: "Azure Policy / OPA"
    checks: []


  - id: 4.4
    text: "CNI Plugin"
    checks:
      - id: 4.4.1
        text: "Ensure latest CNI version is used (Manual)"
        type: "manual"
        remediation: |
          Review the documentation of AWS CNI plugin, and ensure latest CNI version is used.
        scored: false

      - id: 4.4.2
        text: "Ensure that all Namespaces have Network Policies defined (Automated)"
        type: "manual"
        remediation: |
          Follow the documentation and create NetworkPolicy objects as you need them.
        scored: false


  - id: 4.5
    text: "Secrets Management"
    checks:
      - id: 4.5.1
        text: "Prefer using secrets as files over secrets as environment variables (Automated)"
        type: "manual"
        remediation: |
          If possible, rewrite application code to read secrets from mounted secret files, rather than
          from environment variables.
        scored: false

      - id: 4.5.2
        text: "Consider external secret storage (Manual)"
        type: "manual"
        remediation: |
          Refer to the secrets management options offered by your cloud provider or a third-party
          secrets management solution.
        scored: false


  - id: 4.6
    text: "General Policies"
    checks:
      - id: 4.6.1
        text: "Create administrative boundaries between resources using namespaces (Manual)"
        type: "manual"
        remediation: |
          Follow the documentation and create namespaces for objects in your deployment as you need
          them.
        scored: false

      - id: 4.6.2
        text: "Apply Security Context to Your Pods and Containers (Manual)"
        type: "manual"
        remediation: |
          Follow the Kubernetes documentation and apply security contexts to your pods. For a
          suggested list of security contexts, you may refer to the CIS Security Benchmark for Docker
          Containers.
        scored: false

      - id: 4.6.3
        text: "The default namespace should not be used (Automated)"
        type: "manual"
        remediation: |
          Ensure that namespaces are created to allow for appropriate segregation of Kubernetes
          resources and that all new resources are created in a specific namespace.
        scored: false
